# Sanctuary Vault and Consul Information

We run vault and its backend HA consul in the [drud-prod project](https://console.cloud.google.com/home/dashboard?project=drud-prod) on gke.  In the cluster are running two front-end vault instances that are sort-of HA, and 3 back-end consul instances. The full [configuration](https://github.com/drud/vault-consul-on-kube), [README.md](https://github.com/drud/vault-consul-on-kube/blob/master/README.md), and [troubleshooting.md](https://github.com/drud/vault-consul-on-kube/blob/master/troubleshooting.md) are in the github project.

The vault front-ends are referred to as &quot;pseudo-HA&quot; because if one dies (or a node migration happens, or ...) it will come up again _but both will have to be manually unsealed_. Two unseal keys are required to unseal (use the &quot;vault unseal&quot; command).  Chris Sterling, Kevin Bridges, Brad Bowman, Erin Corson, Rick Manelius, and Randy Fay each currently have unseal keys, which can be used separately or together (any two will unseal).

**Monitoring** : The vault front ends are [monitored in the drud-prod project](https://app.google.stackdriver.com/policy-advanced?project=drud-prod) by Stackdriver.  If either one of the vault front-ends is sealed, or if it&#39;s impossible to read a sample value, an alert will be noted in slack #alerts and an email sent to  [ops@drud.com.](mailto:ops@newmediadenver.com.) (The vault token 'monitoring' (yes, that's the actual token) is used to access the sample value for monitoring. It must be renewed periodically, so there's a jenkins job that renews it. If it expires for some reason it can just be recreated with `vault token-create -orphan -id="monitoring" -display-name=monitoring_only -renewable=true -policy=monitoring -period="720h"` .) The consul cluster is monitored by jenkins (below) - it hits the cluster to make sure there&#39;s a leader and pinging #alerts in slack if there&#39;s a problem.  

**Prod jenkins** : A jenkins instance is maintained for miscellaneous uses in drud-prod. To use it, with your kubectl set for drud-prod cluster, To use it, kubectl port-forward jenkins-... 8080 and then hit  [http://localhost:8080/](http://localhost:8080/) and log in. Currently it does 1) Consul backups via snapshot to GCS, 2) Self-backups to GCS, 3) consul monitoring.  Jenkins self-backup tarballs are stored in the [drud\_prod\_jenkins\_backup bucket](https://console.cloud.google.com/storage/browser/drud_prod_jenkins_backup/?project=drud-prod) in drud-prod GCS.  This jenkins instance is configured from [drud/drud-prod-jenkins](https://github.com/drud/drud-prod-jenkins), which has a decent README.

**Consul Backup and Restore** : The vault front-end keeps no state; all state is kept (encrypted) in the consul back-end. Consul backups are made by jenkins via the &quot;snapshot&quot; feature. Backups are stored in GCS in the [drud-prod-consul-backups bucket](https://console.cloud.google.com/storage/browser/drud-prod-consul-backups/?project=drud-prod), and full restore instructions are provided in the [troubleshooting.md](https://github.com/drud/vault-consul-on-kube/blob/master/troubleshooting.md#complete-loss-and-rebuild-with-recovery-using-a-consul-snapshot). This is a drastic recovery technique though, as the normal HA nature of the consul cluster should sort everything out, as described elsewhere in the troubleshooting doc.
